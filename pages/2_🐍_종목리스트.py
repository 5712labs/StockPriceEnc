# Import libraries
# https://sjblog1.tistory.com/41
# http://data.krx.co.kr/contents/MDC/MDI/mdiLoader/index.cmd?menuId=MDC0301
import streamlit as st
import pandas as pd
import numpy as np
from pykrx import stock
import FinanceDataReader as fdr
from datetime import datetime
from dateutil.relativedelta import relativedelta

# Page setup
st.set_page_config(page_title="Python Talks Search Engine", page_icon="ðŸ", layout="wide")
st.title("Stock Search Engine")

clear_button = st.sidebar.button("Clear Cache", key="clear")
if clear_button:
    st.cache_data.clear()

# krx_url = 'https://kind.krx.co.kr/corpgeneral/corpList.do?method=download&searchType=13'
# fdr_datastk_data = pd.read_html(krx_url, header=0)[0]  # í•´ë‹¹ siteì—ì„œ table ì¶”ì¶œ ë° headerëŠ” ê°€ìž¥ ì²«ë²ˆì§¸ í–‰
# st.write(f'fdr_datastk_data {fdr_datastk_data.shape}')
# st.write(fdr_datastk_data)
# stk_data = fdr_datastk_data[['ì¢…ëª©ì½”ë“œ', 'ì—…ì¢…', 'ì£¼ìš”ì œí’ˆ']]
# # ì¢…ëª©ì½”ë“œê°€ ëª¨ë‘ 6ìžë¦¬ë¡œ ì´ë£¨ì–´ì ¸ìžˆì§€ë§Œ í˜¹ì‹œ ëª¨ë¥´ë‹ˆ, 6ìžë¦¬ ë¯¸ë§Œ ì½”ë“œëŠ” ì•žì— 0ì„ ì±„ì›Œë„£ì–´ 6ìžë¦¬ ìˆ«ìží…ìŠ¤íŠ¸ë¡œ ë³€í™˜
# stk_data['ì¢…ëª©ì½”ë“œ'] = stk_data['ì¢…ëª©ì½”ë“œ'].apply(lambda input: '0' * (6 - len(str(input))) + str(input))
# st.write(f'stk_data {stk_data.shape}')
# st.write(stk_data)

# fdr_data = fdr.StockListing("KRX")
# st.write(f'fdr_data {fdr_data.shape}')
# st.write(fdr_data)

# stocks = stock.get_market_fundamental_by_ticker(date=datetime.today(), market="ALL")
# st.write(f'stocks {stocks.shape}')
# st.write(stocks)

# stocks_KS22 = pd.DataFrame({'ì¢…ëª©ì½”ë“œ':stock.get_market_ticker_list(market="KOSDAQ")})
# stocks_KS22['ì¢…ëª©ëª…'] = stocks_KS22['ì¢…ëª©ì½”ë“œ'].map(lambda x: stock.get_market_ticker_name(x))
# st.write(stocks_KS22)


# stocks_KQ22 = pd.DataFrame({'ì¢…ëª©ì½”ë“œ':stock.get_market_ticker_list(market="KOSPI")})
# stocks_KQ22['ì¢…ëª©ëª…'] = stocks_KQ22['ì¢…ëª©ì½”ë“œ'].map(lambda x: stock.get_market_ticker_name(x))
# st.write(stocks_KQ22)

# @st.cache_data
def load_data():
    stocks_KQ = pd.DataFrame({'ì¢…ëª©ì½”ë“œ':stock.get_market_ticker_list(market="KOSPI")})
    stocks_KQ['ì¢…ëª©ëª…'] = stocks_KQ['ì¢…ëª©ì½”ë“œ'].map(lambda x: stock.get_market_ticker_name(x))
    stocks_KQ['ì½”ë“œ'] = stocks_KQ['ì¢…ëª©ì½”ë“œ'] + '.KS'
    
    stocks_KS = pd.DataFrame({'ì¢…ëª©ì½”ë“œ':stock.get_market_ticker_list(market="KOSDAQ")})
    stocks_KS['ì¢…ëª©ëª…'] = stocks_KS['ì¢…ëª©ì½”ë“œ'].map(lambda x: stock.get_market_ticker_name(x))
    stocks_KS['ì½”ë“œ'] = stocks_KS['ì¢…ëª©ì½”ë“œ'] + '.KQ'

    stocks_KQKS = pd.concat([stocks_KQ, stocks_KS])
    
    # ì „ì²´ ì¢…ëª©ì˜ ì‹œê°€ì´ì•¡ 
    # fdr_data = fdr.StockListing("KRX")
    # stocks_KQ['ì‹œì´'] = stocks_KQ['ì¢…ëª©ëª…'].map(lambda x: fdr_data[fdr_data["Name"]==x]["Marcap"].iloc[0])
    
    # ì „ì²´ ì¢…ëª©ì˜ ì—…ì¢… 
    krx_url = 'https://kind.krx.co.kr/corpgeneral/corpList.do?method=download&searchType=13'
    fdr_datastk_data = pd.read_html(krx_url, header=0, encoding='cp949')[0]  # í•´ë‹¹ siteì—ì„œ table ì¶”ì¶œ ë° headerëŠ” ê°€ìž¥ ì²«ë²ˆì§¸ í–‰
    stk_data = fdr_datastk_data[['ì¢…ëª©ì½”ë“œ', 'ì—…ì¢…', 'ì£¼ìš”ì œí’ˆ']]     # 9ê°œì˜ ì—´ ì¤‘ 'íšŒì‚¬ëª…', 'ì¢…ëª©ì½”ë“œ' ë§Œ ì¶”ì¶œí•˜ì—¬ dataframe ì™„ì„±
    # ì¢…ëª©ì½”ë“œê°€ ëª¨ë‘ 6ìžë¦¬ë¡œ ì´ë£¨ì–´ì ¸ìžˆì§€ë§Œ í˜¹ì‹œ ëª¨ë¥´ë‹ˆ, 6ìžë¦¬ ë¯¸ë§Œ ì½”ë“œëŠ” ì•žì— 0ì„ ì±„ì›Œë„£ì–´ 6ìžë¦¬ ìˆ«ìží…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    stk_data['ì¢…ëª©ì½”ë“œ'] = stk_data['ì¢…ëª©ì½”ë“œ'].apply(lambda input: '0' * (6 - len(str(input))) + str(input))
    stock_list = pd.merge(stocks_KQKS, stk_data, on='ì¢…ëª©ì½”ë“œ', how='left')
    # ì „ì²´ ì¢…ëª©ì˜ íŽ€ë”ë©˜íƒˆ ì§€í‘œ ê°€ì ¸ì˜¤ê¸°
    # íŽ€ë”ë©˜íƒˆ ì§€í‘œëŠ” PER, PBR, EPS, BPS, DIV, DPSë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    search_date = datetime.today() - relativedelta(day=1)
    stock_fud = pd.DataFrame(stock.get_market_fundamental_by_ticker(date=search_date, market="ALL"))
    stock_fud = stock_fud.reset_index()
    stock_fud.rename(columns={'í‹°ì»¤':'ì¢…ëª©ì½”ë“œ'}, inplace=True)

    # result = pd.merge(stock_list, stock_fud, left_on='ì¢…ëª©ì½”ë“œ', right_on='ì¢…ëª©ì½”ë“œ', how='left')
    result = pd.merge(stock_list, stock_fud, on='ì¢…ëª©ì½”ë“œ', how='left')
    
    stock_price = stock.get_market_ohlcv_by_ticker(date=search_date, market="ALL")
    stock_price = stock_price.reset_index()
    stock_price.rename(columns={'í‹°ì»¤':'ì¢…ëª©ì½”ë“œ'}, inplace=True)
    # result1 = pd.merge(result, stock_price, left_on='ì¢…ëª©ì½”ë“œ', right_on='ì¢…ëª©ì½”ë“œ', how='left')
    result1 = pd.merge(result, stock_price, on='ì¢…ëª©ì½”ë“œ', how='left')

    #ì½”ë„¥ìŠ¤ ì œê±°
    result1.dropna(subset=['ì¢…ëª©ëª…'], how='any', axis=0, inplace=True)
    
    # result1 = result1.replace([0], np.nan)    # 0ê°’ì„ NaNìœ¼ë¡œ ë³€ê²½
    # result1 = result1.dropna(axis=0)      # NaNì„ ê°€ì§„ í–‰ ì œê±°
    result1 = result1.sort_values(by=['PER'], ascending=True)
    result1['ë‚´ìž¬ê°€ì¹˜'] = (result1['BPS'] + (result1['EPS']) * 10) / 2
    result1['ë‚´ìž¬ê°€ì¹˜/ì¢…ê°€'] = (result1['ë‚´ìž¬ê°€ì¹˜'] / result1['ì¢…ê°€'])
    # st.write('result1')
    # st.write(result1.head())
    return result1

analy = load_data()

with st.expander(f"ì „ì²´ ë³´ê¸°{analy.shape}"):
    st.write(analy)

text_search = st.text_input("ì¢…ëª©ì½”ë“œ, ì¢…ëª©ëª…, ì—…ì¢…, ì£¼ìš”ì œí’ˆìœ¼ë¡œ ê²€ìƒ‰í•˜ì„¸ìš”", value="")
# Filter the dataframe using masks
m1 = analy["ì¢…ëª©ì½”ë“œ"].str.contains(text_search)
m2 = analy["ì¢…ëª©ëª…"].str.contains(text_search)
m3 = analy["ì—…ì¢…"].str.contains(text_search)
m4 = analy["ì£¼ìš”ì œí’ˆ"].str.contains(text_search)
# if text_search.contains('PER'):
# if text_search in 'PER':
#     m5 = analy["PER"] > 10
# else:
#     m5 = analy["PER"] < 10

# df_search = analy[m1 | m2 | m3 | m4 | m5]
df_search = analy[m1 | m2 | m3 | m4]
# df_search = analy[m3 | m4]
if text_search:
    st.write(df_search)

# Another way to show the filtered results
# Show the cards
N_cards_per_row = 6
if text_search:
    for n_row, row in df_search.reset_index().iterrows():
        i = n_row%N_cards_per_row
        if i==0:
            st.write("---")
            cols = st.columns(N_cards_per_row, gap="small")
        # draw the card
        with cols[n_row%N_cards_per_row]:
            # st.button()
            # st.caption(f"{row['Evento'].strip()} - {row['Lugar'].strip()} - {row['Fecha'].strip()} ")
            st.button(f"**{row['ì¢…ëª©ëª…']}**", type="secondary")
            # clear_button = st.sidebar.button("Clear Cache", key="clear")
            # if clear_button:
            #     st.cache_data.clear()
            st.caption(f"{row['ì½”ë“œ']}")
            st.markdown(f"****{row['ì£¼ìš”ì œí’ˆ']}****")
            st.markdown(f"**{row['ì¢…ëª©ì½”ë“œ']}**")
            # st.markdown(f"*{row['ì¢…ëª©ëª…'].strip()}*")

# if st.button("Confirm"):
#     con = st.container()
#     con.caption("Result")
    # con.write(f"Hello~ {str(input_user_name)}")

st.stop()



# Connect to the Google Sheet
sheet_id = "1nctiWcQFaB5UlIs6z8d1O6ZgMHFDMAoo3twVxYnBUws"
sheet_name = "charlas"
url = f"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}"
df = pd.read_csv(url, dtype=str).fillna("")

# Show the dataframe (we'll delete this later)
st.write(df)

text_search = st.text_input("Search videos by title or speaker", value="")
# Filter the dataframe using masks
m1 = df["Autor"].str.contains(text_search)
m2 = df["TÃ­tulo"].str.contains(text_search)
df_search = df[m1 | m2]
# Show the results, if you have a text_search
if text_search:
    st.write(df_search)
# Another way to show the filtered results
# Show the cards
N_cards_per_row = 3
if text_search:
    for n_row, row in df_search.reset_index().iterrows():
        i = n_row%N_cards_per_row
        if i==0:
            st.write("---")
            cols = st.columns(N_cards_per_row, gap="large")
        # draw the card
        with cols[n_row%N_cards_per_row]:
            st.caption(f"{row['Evento'].strip()} - {row['Lugar'].strip()} - {row['Fecha'].strip()} ")
            st.markdown(f"**{row['Autor'].strip()}**")
            st.markdown(f"*{row['TÃ­tulo'].strip()}*")
            st.markdown(f"**{row['Video']}**")