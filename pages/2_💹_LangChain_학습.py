# pip install langchain=0.0.142
# pip install openai=0.27.4
# pip install tiktoken=0.3.3
# pip install pinecone-client
# https://blog.futuresmart.ai/building-a-document-based-question-answering-system-with-langchain-pinecone-and-llms-like-gpt-4-and-chatgpt#heading-7-embedding-documents-with-openai
import streamlit as st
import os
import pinecone
from langchain.vectorstores import Chroma
from langchain.embeddings.openai import OpenAIEmbeddings 
from langchain.text_splitter import RecursiveCharacterTextSplitter 
from langchain.document_loaders import TextLoader
from langchain.document_loaders import DirectoryLoader
from langchain.vectorstores import Pinecone
import re
import convert
import pandas as pd
import numpy as np
import time

title = 'LLM Learning'
st.set_page_config(page_title="title", page_icon="ğŸ", layout="wide")
st.title(title)

if convert.check_password() == False:
    st.stop()

os.environ['OPENAI_API_KEY'] = st.secrets["api_key"]

tab1, tab2, tab3, tab4 , tab5 = st.tabs(
    ["í•™ìŠµ(txt)_Pinecone", 
    "ì „ì²´ëª©ë¡_Pinecone",
    "ğŸ—ƒ í•™ìŠµ(txt)_Chroma",
    "ì „ì²´ëª©ë¡_Chroma",
    "í•™ìŠµ(csv)_FAISS"]
    )

with tab1:
    st.header("í•™ìŠµ(txt)_Pinecone")
    pinecone.init(api_key=f"{st.secrets['api_pine']}", environment='gcp-starter')
    index_name = 'dwlangchain'
    # st.write('pinecone.list_indexes()')
    # st.write(pinecone.list_indexes())
    # loader = DirectoryLoader('./sources', glob='*.txt', loader_cls=TextLoader)
    # loader = DirectoryLoader('./sources', glob='DTSM-IR-203_011.txt', loader_cls=TextLoader)
    loader = DirectoryLoader('./sources', glob='plant.txt', loader_cls=TextLoader)
    documents = loader.load()

    # text ì •ì œ
    output = []
    # https://study-easy-coding.tistory.com/67
    for page in documents:
        text = page.page_content
        text = re.sub(r'(\w+)-\n((\w+))', r'\1\2', text) # ì•ˆë…•-\ní•˜ì„¸ìš” -> ì•ˆë…•í•˜ì„¸ìš”
        text = re.sub(r'(?<!\n\s)\n(?!\s\n)', ' ' , text.strip()) # "ì¸\nê³µ\n\nì§€ëŠ¥íŒ©í† ë¦¬ -> ì¸ê³µì§€ëŠ¥íŒ©í† ë¦¬
        text = re.sub(r'\n\s*\n', '\n\n' , text) # "\në²„\n\nê±°\n\ní‚¹\n => ë²„\nê±°\ní‚¹
        delete_word = re.sub(r'[\(]+[\w\s]+[\)]+','',text) #ìŠ¤íŠ¸ë§ ë³‘ê¸°ë¬¸ìë¥¼ ì‚­ì œí•˜ê³  ê·¸ ê°’ì„ replace_wordì— ìŠ¤íŠ¸ë§ìœ¼ë¡œ ë‹´ê¹€
        output.append(text)
    # st.write(documents)
    # st.write(output)

    # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬(chunk) ë‹¨ìœ„ë¡œ ë¶„í• í•˜ê¸°
    chunk_size = 1000
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=20)
    texts = text_splitter.split_documents(documents)
    st.write(f'{len(documents)}ê°œì˜ ë¬¸ì„œë¥¼ {chunk_size} ì²­í¬ ë‹¨ìœ„ë¡œ {len(texts)}ê°œì˜ ë¬¸ì„œë¡œ ë¶„í•  í•˜ì˜€ìŠµë‹ˆë‹¤.')
    st.write(texts)

    upsert_button = st.button("Upsert to Pinecone DB", key="upsertPinecone", type='primary')
    if upsert_button:
        # index = pinecone.Index("dwlangchain")
        index_name = 'dwlangchain'
        embedding = OpenAIEmbeddings()
        index = Pinecone.from_documents(texts, embedding, index_name=index_name)
        st.info(f""" 
        ### ì—…ë¡œë“œë¥¼ ì™„ë£Œí•˜ì˜€ìŠµë‹ˆë‹¤.
        #### Pinecone
        [https://app.pinecone.io/](https://app.pinecone.io/).
        """)

with tab2:

    pinecone.init(api_key=f"{st.secrets['api_pine']}", environment='gcp-starter')
    index_name = 'dwlangchain'

    st.info(f""" 
        ###### Pinecone
        [https://app.pinecone.io/](https://app.pinecone.io/)
        """)
    
    def all_ids_data():
        indexquery = pinecone.Index(index_name)
        namespace = ''
        if len(indexquery.describe_index_stats()["namespaces"]) == 0:
            st.write('ë°ì´í„°ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.')
            st.stop()
        num_vectors = indexquery.describe_index_stats()["namespaces"][namespace]['vector_count']
        st.write(num_vectors)

        num_dimensions = 1536
        all_ids_df = pd.DataFrame()
        while len(all_ids_df) < num_vectors:
            input_vector = np.random.rand(num_dimensions).tolist()
            results = indexquery.query(
                vector=input_vector, 
                top_k=10000,
                include_values=False,
                include_metadata=True
                )
            for result in results['matches']:
                ids_df = pd.DataFrame([[result['id'], result['metadata']['text'], result['metadata']['source']]])
                all_ids_df = pd.concat([all_ids_df, ids_df])
        all_ids_df.reset_index(drop=True, inplace=True)
        all_ids_df.columns = ['id', 'text', 'source']
        return all_ids_df

    with st.spinner('Wait for it...'):
        all_ids_df = all_ids_data().sort_values(by=['text'], ascending=True)
        st.write(all_ids_df)

        # ë²¡í„° ì¤‘ë³µê°’ ì œê±°
        delete_dup_ids_button = st.button("ì¤‘ë³µê°’ ì œê±°", key="deleteadup", type='primary')
        if delete_dup_ids_button:
            all_ids_df = all_ids_data().sort_values(by=['text'], ascending=True)
            dup = all_ids_df.duplicated(['text'], keep='first')
            all_dup_df = pd.concat([all_ids_df, dup], axis=1)
            all_dup_df.rename(columns = {0 : 'dup'}, inplace = True)
            st.write(all_dup_df)
            remain_first_df = all_dup_df[all_dup_df['dup'] == True]['id'].values.tolist()
            st.write(remain_first_df)
            indexquery = pinecone.Index(index_name)
            indexquery.delete(ids=remain_first_df, namespace='')
            st.info(f""" ì¤‘ë³µê°’ì„ ì œê±°í•˜ì˜€ìŠµë‹ˆë‹¤. """)  
            time.sleep(2)
        
        # ë²¡í„° ì „ì²´ê°’ ì œê±°
        delete_all_ids_button = st.button("ì „ì²´ê°’ ì œê±°", key="deleteall", type='primary')
        if delete_all_ids_button:
            all_ids_df = all_ids_data()
            all_ids = all_ids_df['id'].values.tolist()
            indexquery = pinecone.Index(index_name)
            indexquery.delete(ids=all_ids, namespace='')
            st.info(f""" ì „ì²´ ë°ì´í„°ë¥¼ ì œê±°í•˜ì˜€ìŠµë‹ˆë‹¤. """)
            time.sleep(2)

with tab3:
    # st.header(tab2.title)
    # loader = GutenbergLoader("https://www.gutenberg.org/cache/epub/69972/pg69972.txt")
    # loader = DirectoryLoader('./sources', glob='*.txt', loader_cls=TextLoader)
    # loader = DirectoryLoader('./sources', glob='DTSM-IR-203_011.txt', loader_cls=TextLoader)
    loader = DirectoryLoader('./sources', glob='plant.txt', loader_cls=TextLoader)
    documents = loader.load()

    # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬(chunk) ë‹¨ìœ„ë¡œ ë¶„í• í•˜ê¸°
    chunk_size = 1000
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=20)
    texts = text_splitter.split_documents(documents)
    st.write(f'{len(documents)}ê°œì˜ ë¬¸ì„œë¥¼ {chunk_size} ì²­í¬ ë‹¨ìœ„ë¡œ {len(texts)}ê°œì˜ ë¬¸ì„œë¡œ ë¶„í•  í•˜ì˜€ìŠµë‹ˆë‹¤.')
    st.write(texts)

    upsert_button = st.button("Upsert to Chroma Local DB", key="upsertChroma", type='primary')
    if upsert_button:
        # persist_directory="/content/drive/My Drive/Colab Notebooks/chroma/romeo"
        persist_directory="db"
        # os.environ['OPENAI_API_KEY'] = st.secrets["api_key"]
        embedding = OpenAIEmbeddings()
        vectordb = Chroma.from_documents(
            documents=texts,
            embedding=embedding, 
            persist_directory=persist_directory)  
        vectordb.persist()
        vectordb = None
        st.info(f""" 
        ### ì—…ë¡œë“œë¥¼ ì™„ë£Œí•˜ì˜€ìŠµë‹ˆë‹¤.
        #### Chroma
        [https://docs.trychroma.com/getting-started/](https://docs.trychroma.com/getting-started/)
        """)



with tab4:
    # ë¶ˆëŸ¬ì˜¤ê¸°
    persist_directory="db"
    embedding = OpenAIEmbeddings()
    vectordb = Chroma(
        embedding_function=embedding, 
        persist_directory=persist_directory)  
    
    st.write(vectordb.get())
    st.write(vectordb.get().keys())
    st.write(len(vectordb.get()["ids"]))
    # Using embedded DuckDB with persistence: data will be stored in: ./chroma_db
    # dict_keys(['ids', 'embeddings', 'documents', 'metadatas'])
    # 7580

    # retriever = vectordb.as_retriever()
    # retriever = vectordb.as_retriever(search_kwargs={"k": 3})
    # docs = retriever.get_relevant_documents("í‡´ì‚¬ì ìˆ˜ ì•Œë ¤ì¤˜")

    # for doc in docs:
    #     st.write(doc.metadata["source"])

# st.write(pinecone.list_indexes())
# st.write(index)

# Upsert sample data (5 8-dimensional vectors)
# index.upsert([
#     ("A", [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]),
#     ("E", [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5])
# ])

# index_name = pinecone.Index("dwlangchain")


# import openai

# # get api key from platform.openai.com
# openai.api_key = os.getenv('OPENAI_API_KEY') or 'OPENAI_API_KEY'

# embed_model = "text-embedding-ada-002"
# query = (
#     "Which training method should I use for sentence transformers when " +
#     "I only have pairs of related sentences?"
# )
# res = openai.Embedding.create(
#     input=[query],
#     engine=embed_model
# )

# # retrieve from Pinecone
# xq = res['data'][0]['embedding']
# # get relevant contexts (including the questions)
# res = index.query(xq, top_k=2, include_metadata=True)


with tab5:
    from langchain.document_loaders.csv_loader import CSVLoader
    from langchain.vectorstores import FAISS
    from langchain.prompts import PromptTemplate
    from langchain.chat_models import ChatOpenAI
    from langchain.chains import LLMChain
    # loader = DirectoryLoader('./sources', glob='DTSM-PU-310_002.txt', loader_cls=TextLoader)
    loader = CSVLoader(file_path="./sources/sales_response.csv")
    documents = loader.load()

    st.write(documents)
    embeddings = OpenAIEmbeddings()
    db = FAISS.from_documents(documents, embeddings)


    def retrieve_info(query):
        similar_response = db.similarity_search(query, k=3)
        page_contents_array = [doc.page_content for doc in similar_response]
        # print(page_contents_array)
        return page_contents_array

    # 3. Setup LLMChain & prompts
    llm = ChatOpenAI(temperature=0, model="gpt-3.5-turbo-16k-0613")

    template = """
    You are a world class business development representative. 
    I will share a prospect's message with you and you will give me the best answer that 
    I should send to this prospect based on past best practies, 
    and you will follow ALL of the rules below:

    1/ Response should be very similar or even identical to the past best practies, 
    in terms of length, ton of voice, logical arguments and other details

    2/ If the best practice are irrelevant, then try to mimic the style of the best practice to prospect's message

    Below is a message I received from the prospect:
    {message}

    Here is a list of best practies of how we normally respond to prospect in similar scenarios:
    {best_practice}

    Please write the best response that I should send to this prospect:
    """

    prompt = PromptTemplate(
        input_variables=["message", "best_practice"],
        template=template
    )
    st.write(prompt)
    chain = LLMChain(llm=llm, prompt=prompt)


    # 4. Retrieval augmented generation
    def generate_response(message):
        best_practice = retrieve_info(message)
        response = chain.run(message=message, best_practice=best_practice)
        return response
    st.write('5712')
    message = st.text_area("customer message")

    if message:
        st.write("Generating best practice message...")

        result = generate_response(message)

        st.info(result)

